# import python packages
import os
from abc import ABC, abstractmethod
import numpy as np
import scipy as sp
import pandas as pd
import calendar
import collections
from decimal import Decimal
from inspect import currentframe, getframeinfo
import matplotlib.pyplot as plt

# NPL packages
import punpy
import comet_maths as cm

# HCP files
from Source import PATH_TO_CONFIG
from Source.Utilities import Utilities
from Source.ConfigFile import ConfigFile
from Source.HDFRoot import HDFRoot  # for typing
from Source.HDFGroup import HDFGroup  # for typing
from Source.ProcessL1b_FRMCal import ProcessL1b_FRMCal
from Source.Uncertainty_Analysis import Propagate
from Source.CalibrationFileReader import CalibrationFileReader
from Source.ProcessL1b_FactoryCal import ProcessL1b_FactoryCal


class Instrument(ABC):
    """Base class for instrument uncertainty analysis"""

    def __init__(self):
        pass

    @abstractmethod
    def lightDarkStats(self, grp: HDFGroup, slice: list, sensortype: str) -> dict[np.array]:
        """
        :param grp: HDFGroup representing the sensor specific data
        :param slice: Sliced sensor data
        :param sensortype: sensor name

        :return:
        """

        pass

    def generateSensorStats(self, InstrumentType: str, rawData: dict, rawSlice: dict, newWaveBands: np.array)\
            -> dict[np.array]:
        """
        :return: dictionary of statistics used later in the processing pipeline. Keys are:
        [ave_Light, ave_Dark, std_Light, std_Dark, std_Signal]
        """
        output = {}  # used tp store standard deviations and averages as a function return for generateSensorStats
        types = ['ES', 'LI', 'LT']
        for sensortype in types:
            if InstrumentType.lower() == "trios":
                # rawData is the group and used to access _CAL, _BACK, and other information about the
                # DarkPixels... not entirely clear.
                output[sensortype] = self.lightDarkStats(rawData[sensortype], rawSlice[sensortype], sensortype)
            elif InstrumentType.lower() == "seabird":
                # rawData here is the group, passed along only for the purpose of
                # confirming "FrameTypes", i.e., ShutterLight or ShutterDark. Calculations
                # are performed on the Slice.
                # output contains:
                # ave_Light: (array 1 x number of wavebands)
                # ave_Dark: (array 1 x number of wavebands)
                # std_Light: (array 1 x number of wavebands)
                # std_Dark: (array 1 x number of wavebands)
                # std_Signal: OrdDict by wavebands: sqrt( (std(Light)^2 + std(Dark)^2)/ave(Light)^2 )
                output[sensortype] = self.lightDarkStats([rawData[sensortype]['LIGHT'],
                                                          rawData[sensortype]['DARK']],
                                                         [rawSlice[sensortype]['LIGHT'],
                                                          rawSlice[sensortype]['DARK']],
                                                         sensortype)
        if not output[sensortype]:
            msg = "Error in generating standard deviation and average of light and dark"
            print(msg)
            return False

        # interpolate std Signal to common wavebands for use in band convolution
        for stype in types:
            _, output[stype]['std_Signal_Interpolated'] = self.interp_common_wvls(
                output[stype]['std_Signal'],
                np.asarray(list(output[stype]['std_Signal'].keys()), dtype=float),
                newWaveBands)

        return output


    def Factory(self, node: HDFRoot, uncGrp: HDFGroup, stats: dict) -> dict[np.array]:
        """

        :param node: HDFRoot which stores the L1BQC data for L2Processing
        :param uncGrp: HDFGroup which contains the uncertainty budget, all input uncertainties
        :param stats: dictionary generated by Source.ProcessInstrumentUncertainties.generateSensorStats() contains
        sensor specific standard deviations and averages for the light, dark and light-dark signals.

        :return: dictionary of output instrument uncertainties [ES, LI, LT]
        """
        # read in uncertainties from HDFRoot and define propagate object
        PropagateL1B = Propagate(M=100, cores=0)

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        # loop through instruments
        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_UNC") # SIRREX data
            radcal.datasetToColumns()

            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            ### ADERU : Read radiometric coeff value from configuration files
            Cal[sensor] = np.asarray(list(radcal.columns['unc']))
            calFolder = os.path.splitext(ConfigFile.filename)[0] + "_Calibration"
            calPath = os.path.join(PATH_TO_CONFIG, calFolder)
            calibrationMap = CalibrationFileReader.read(calPath)
            waves, Coeff[sensor] = ProcessL1b_FactoryCal.extract_calibration_coeff(node, calibrationMap, sensor)

            if waves is None and Coeff is None:
                msg = 'ProcessInstrumentUncertainties Fail'
                print(msg)
                Utilities.writeLogFile(msg)
                return None

            # TODO: temporary fix angular for ES is written as ES_POL
            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        ones = np.ones(len(Cal['ES']))  # to provide array of 1s with the correct shape

        # create lists containing mean values and their associated uncertainties (list order matters)
        # if ConfigFile.settings['SensorType'].lower() == "trios":
        #     # for trios the dark average and std is one value
        #     esaveDark = ones*stats['ES']['ave_Dark']
        #     liaveDark = ones*stats['LI']['ave_Dark']
        #     ltaveDark = ones*stats['LT']['ave_Dark']
        #     esstdDark = ones*stats['ES']['std_Dark']
        #     listdDark = ones*stats['LI']['std_Dark']
        #     ltstdDark = ones*stats['LT']['std_Dark']
        #
        # elif ConfigFile.settings['SensorType'].lower() == "seabird":
        #     # for seabird it is an array of length 255
        #     esaveDark = stats['ES']['ave_Dark']
        #     liaveDark = stats['LI']['ave_Dark']
        #     ltaveDark = stats['LT']['ave_Dark']
        #     esstdDark = stats['ES']['std_Dark']
        #     listdDark = stats['LI']['std_Dark']
        #     ltstdDark = stats['LT']['std_Dark']

        mean_values = [stats['ES']['ave_Light'], stats['ES']['ave_Dark'],
                       stats['LI']['ave_Light'], stats['LI']['ave_Dark'],
                       stats['LT']['ave_Light'], stats['LT']['ave_Dark'],
                       Coeff['ES'], Coeff['LI'], Coeff['LT'],
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones]

        uncertainty = [stats['ES']['std_Light'], stats['ES']['std_Dark'],
                       stats['LI']['std_Light'], stats['LI']['std_Dark'],
                       stats['LT']['std_Light'], stats['LT']['std_Dark'],
                       Cal['ES']*Coeff['ES']/200, Cal['LI']*Coeff['LI']/200, Cal['LT']*Coeff['LT']/200,
                       cStab['ES'], cStab['LI'], cStab['LT'],
                       cLin['ES'], cLin['LI'], cLin['LT'],
                       np.array(cStray['ES'])/100, np.array(cStray['LI'])/100, np.array(cStray['LT'])/100,
                       np.array(Ct['ES']), np.array(Ct['LI']), np.array(Ct['LT']),
                       np.array(cPol['LI']), np.array(cPol['LT']), np.array(cPol['ES'])]

        # generate uncertainties using Monte Carlo Propagation (M=100, def line 27)

        # NOTE: ISSUE #95
        ES_unc, LI_unc, LT_unc = PropagateL1B.propagate_Instrument_Uncertainty(mean_values, uncertainty)
        es, li, lt = PropagateL1B.instruments(*mean_values)
        ES_unc = ES_unc/es  # convert to relative uncertainty
        LI_unc = LI_unc/li  # convert to relative uncertainty
        LT_unc = LT_unc/lt  # convert to relative uncertainty

        # return uncertainties as dictionary to be appended to xSlice
        data_wvl = np.asarray(list(stats['ES']['std_Signal_Interpolated'].keys()), dtype=float)  # std_Signal_Interpolated has keys which represent common wavebands for ES, LI, & LT.
        _, es_Unc = self.interp_common_wvls(ES_unc,
                                            np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                            data_wvl)
        _, li_Unc = self.interp_common_wvls(LI_unc,
                                            np.array(uncGrp.getDataset("LI_RADCAL_UNC").columns['wvl'], dtype=float),
                                            data_wvl)
        _, lt_Unc = self.interp_common_wvls(LT_unc,
                                            np.array(uncGrp.getDataset("LT_RADCAL_UNC").columns['wvl'], dtype=float),
                                            data_wvl)

        # NOTE: Commented the following as SeaBird Factory does not contain _RADCAL_CAL datasets in the RAW_UNCERTAINTIES group
        # radcal_cal = pd.DataFrame(uncGrp.getDataset(sensor + "_RADCAL_CAL").data)['2']

        # ind_zero = radcal_cal <= 0
        # ind_nan = np.isnan(radcal_cal)
        # ind_nocal = ind_nan | ind_zero

        # es_Unc[ind_nocal == True] = 0
        # li_Unc[ind_nocal == True] = 0
        # lt_Unc[ind_nocal == True] = 0

        return dict(
            esUnc=es_Unc,
            liUnc=li_Unc,
            ltUnc=lt_Unc,
        )

    def Default(self, uncGrp: HDFGroup, stats: dict) -> dict[str, np.array]:
        """

        :param uncGrp: HDFGroup which contains the uncertainty budget, all imput uncertainties
        :param stats: dictionary generated by Source.ProcessInstrumentUncertainties.generateSensorStats() contains
        sensor specific standard deviations and averages for the light, dark and light-dark signals.

        :return: dictionary of output instrument uncertainties [Es_unc, Li_unc, Lt_unc]
        """
        # read in uncertainties from HDFRoot and define propagate object
        PropagateL1B = Propagate(M=100, cores=0)

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        # loop through instruments
        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_CAL")
            radcal.datasetToColumns()

            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            if ConfigFile.settings['SensorType'].lower() == "trios":
                # Convert TriOS mW/m2/nm to uW/cm^2/nm
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))/10
            elif ConfigFile.settings['SensorType'].lower() == "seabird":
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))
            Cal[sensor] = np.asarray(list(radcal.columns['3']))

            # TODO: temporary fix angular for ES is written as ES_POL
            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        ones = np.ones(len(Cal['ES']))  # to provide array of 1s with the correct shape
        # zeros = np.zeros(len(Cal['ES']))  # for testing

        # create lists containing mean values and their associated uncertainties (list order matters)
        mean_values = [stats['ES']['ave_Light'], stats['ES']['ave_Dark'],
                       stats['LI']['ave_Light'], stats['LI']['ave_Dark'],
                       stats['LT']['ave_Light'], stats['LT']['ave_Dark'],
                       Coeff['ES'], Coeff['LI'], Coeff['LT'],
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones,
                       ones, ones, ones
                       ]

        uncertainty = [stats['ES']['std_Light'], stats['ES']['std_Dark'],
                       stats['LI']['std_Light'], stats['LI']['std_Dark'],
                       stats['LT']['std_Light'], stats['LT']['std_Dark'],
                       Cal['ES']*Coeff['ES']/200, Cal['LI']*Coeff['LI']/200, Cal['LT']*Coeff['LT']/200,
                       cStab['ES'], cStab['LI'], cStab['LT'],
                       cLin['ES'], cLin['LI'], cLin['LT'],
                       np.array(cStray['ES'])/100, np.array(cStray['LI'])/100, np.array(cStray['LT'])/100,
                       np.array(Ct['ES']), np.array(Ct['LI']), np.array(Ct['LT']),
                       np.array(cPol['LI']), np.array(cPol['LT']), np.array(cPol['ES'])
                       ]

        # generate uncertainties using Monte Carlo Propagation (M=100, def line 27)
        es_unc, li_unc, lt_unc = PropagateL1B.propagate_Instrument_Uncertainty(mean_values, uncertainty)
        es, li, lt = PropagateL1B.instruments(*mean_values)
        ES_unc = es_unc / es  # convert to relative uncertainty
        LI_unc = li_unc / li  # convert to relative uncertainty
        LT_unc = lt_unc / lt  # convert to relative uncertainty

        # return uncertainties as dictionary to be appended to xSlice
        data_wvl = np.asarray(list(stats['ES']['std_Signal_Interpolated'].keys()), dtype=float)  # std_Signal_Interpolated has keys which represent common wavebands for ES, LI, & LT.
        _, es_Unc = self.interp_common_wvls(ES_unc,
                                            np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),
                                            data_wvl)
        _, li_Unc = self.interp_common_wvls(LI_unc,
                                            np.array(uncGrp.getDataset("LI_RADCAL_CAL").columns['1'], dtype=float),
                                            data_wvl)
        _, lt_Unc = self.interp_common_wvls(LT_unc,
                                            np.array(uncGrp.getDataset("LT_RADCAL_CAL").columns['1'], dtype=float),
                                            data_wvl)

        radcal_cal = pd.DataFrame(uncGrp.getDataset(sensor + "_RADCAL_CAL").data)['2']

        ind_zero = radcal_cal <= 0
        ind_nan = np.isnan(radcal_cal)
        ind_nocal = ind_nan | ind_zero

        for i, k in enumerate(es_Unc.keys()):
            if ind_nocal[i]:
                es_Unc[k] = [0.0]
                li_Unc[k] = [0.0]
                lt_Unc[k] = [0.0]

        return dict(
            esUnc=es_Unc,
            liUnc=li_Unc,
            ltUnc=lt_Unc,
        )

    @abstractmethod
    def FRM(self, node: HDFRoot, uncGrp: HDFGroup, raw_grps: dict[str, HDFGroup], raw_slices: dict[str, np.array],
            stats: dict, newWaveBands: np.array, refData: dict) -> dict[str, np.array]:
        """
        :param node: HDFRoot of L1BQC data for procressing
        :param uncGrp: HDFGroup of uncertainty budget
        :param raw_grps: dictionary of raw data groups
        :param raw_slices: dictionary of sliced data for specific sensors
        :param stats: standard deviation and averages for Light, Dark and Light-Dark signal
        :param newWaveBands: wavelength subset for interpolation

        :return: output FRM uncertainties
        """
        pass

    ## L2 uncertainty Processing
    @staticmethod
    def rrsHyperUNCFRM(rhoScalar: float, rhoVec: np.array, rhoDelta: np.array, waveSubset: np.array,
                       xSlice: dict[str, np.array]) -> dict[str, np.array]:
        """
        :param rhoScalar: rho input if Mobley99 or threeC rho is used
        :param rhoVec: rho input if Zhang17 rho is used
        :param rhoDelta: uncertainties associated with rho
        :param waveSubset: wavelength subset for any band convolution (and sizing rhoScalar if used)
        :param xSlice: Dictionary of input radiance, raw_counts, standard deviations etc.

        :return: dictionary of output uncertainties that are generated

        """
        # organise data
        # cut data down to wavelengths where rho values exist -- should be no change for M99
        esSampleXSlice = np.asarray([{key: sample for key, sample in
                                      xSlice['esSample'][i].items() if float(key) in waveSubset}
                                     for i in range(len(xSlice['esSample']))])
        liSampleXSlice = np.asarray([{key: sample for key, sample in
                                      xSlice['liSample'][i].items() if float(key) in waveSubset}
                                     for i in range(len(xSlice['liSample']))])
        ltSampleXSlice = np.asarray([{key: sample for key, sample in
                                      xSlice['ltSample'][i].items() if float(key) in waveSubset}
                                     for i in range(len(xSlice['ltSample']))])

        if rhoScalar is not None:  # make rho a constant array if scalar
            rho = np.ones(len(waveSubset))*rhoScalar  # convert rhoScalar to the same dims as other values/Uncertainties
        else:
            rho = np.asarray(list(rhoVec.values()), dtype=float)

        # initialise punpy propagation object
        mdraws = esSampleXSlice.shape[0]  # keep no. of monte carlo draws consistent
        Propagate_L2_FRM = punpy.MCPropagation(mdraws, parallel_cores=1)

        # get sample for rho
        rhoSample = cm.generate_sample(mdraws, rho, rhoDelta, "syst")  # removed *rho because rhoDelta should be in abs units

        # initialise lists to store uncertainties per replicate

        esSample = np.asarray([[i[0] for i in k.values()] for k in esSampleXSlice])  # recover original shape of samples
        liSample = np.asarray([[i[0] for i in k.values()] for k in liSampleXSlice])
        ltSample = np.asarray([[i[0] for i in k.values()] for k in ltSampleXSlice])

        sample_wavelengths = cm.generate_sample(mdraws, np.array(waveSubset), None, None)  # no uncertainty in wvls
        sample_Lw = Propagate_L2_FRM.run_samples(Propagate.Lw_FRM, [ltSample, rhoSample, liSample])
        sample_Rrs = Propagate_L2_FRM.run_samples(Propagate.Rrs_FRM, [ltSample, rhoSample, liSample, esSample])

        output = {}

        if ConfigFile.settings["bL2WeightSentinel3A"]:
            sample_lw_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [sample_Lw, sample_wavelengths])
            sample_rrs_S3A = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3A, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_S3A)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_S3A)

            output["lwUNC_Sentinel3A"] = lwDeltaBand
            output["rrsUNC_Sentinel3A"] = rrsDeltaBand

        if ConfigFile.settings["bL2WeightSentinel3B"]:
            sample_lw_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [sample_Lw, sample_wavelengths])
            sample_rrs_S3B = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_S3B, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_S3B)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_S3B)

            output["lwUNC_Sentinel3B"] = lwDeltaBand
            output["rrsUNC_Sentinel3B"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightMODISA']:
            sample_lw_AQUA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [sample_Lw, sample_wavelengths])
            sample_rrs_AQUA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_AQUA, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_AQUA)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_AQUA)

            output["lwUNC_MODISA"] = lwDeltaBand
            output["rrsUNC_MODISA"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightMODIST']:
            sample_lw_TERRA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA, [sample_Lw, sample_wavelengths])
            sample_rrs_TERRA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_TERRA, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_TERRA)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_TERRA)

            output["lwUNC_MODIST"] = lwDeltaBand
            output["rrsUNC_MODIST"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightVIIRSN']:
            sample_lw_NOAA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA, [sample_Lw, sample_wavelengths])
            sample_rrs_NOAA = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_NOAA)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_NOAA)

            output["lwUNC_VIIRSN"] = lwDeltaBand
            output["rrsUNC_VIIRSN"] = rrsDeltaBand

        if ConfigFile.settings['bL2WeightVIIRSJ']:
            # currently the same as VIIRSN due to the lack of NOAA-21 rsr in pyspectral
            sample_lw_NOAAJ = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA, [sample_Lw, sample_wavelengths])
            sample_rrs_NOAAJ = Propagate_L2_FRM.run_samples(Propagate.band_Conv_Sensor_NOAA, [sample_Rrs, sample_wavelengths])

            lwDeltaBand = Propagate_L2_FRM.process_samples(None, sample_lw_NOAAJ)
            rrsDeltaBand = Propagate_L2_FRM.process_samples(None, sample_rrs_NOAAJ)

            output["lwUNC_VIIRSJ"] = lwDeltaBand
            output["rrsUNC_VIIRSJ"] = rrsDeltaBand

        lwDelta = Propagate_L2_FRM.process_samples(None, sample_Lw)
        rrsDelta = Propagate_L2_FRM.process_samples(None, sample_Rrs)

        output["lwUNC"] = lwDelta  # Multiply by large number to reduce round off error
        output["rrsUNC"] = rrsDelta

        return output

    def rrsHyperUNC(self, uncGrp: HDFGroup, rhoScalar: float, rhoVec: np.array, rhoDelta: np.array,
                    waveSubset: np.array, xSlice: dict[str, np.array]) -> dict[str, np.array]:
        """
        :param uncGrp: HDFGroup storing the uncertainty budget
        :param rhoScalar: rho input if Mobley99 or threeC rho is used
        :param rhoVec: rho input if Zhang17 rho is used
        :param rhoDelta: uncertainties associated with rho
        :param waveSubset: wavelength subset for any band convolution (and sizing rhoScalar if used)
        :param xSlice: Dictionary of input radiance, raw_counts, standard deviations etc.

        :return: dictionary of output uncertainties that are generated
        """

        waveSubset = np.array(waveSubset, dtype=float)  # convert waveSubset to numpy array
        esXstd = xSlice['esSTD_RAW']
        liXstd = xSlice['liSTD_RAW']
        ltXstd = xSlice['ltSTD_RAW']

        es, _ = self.interp_common_wvls(np.asarray(list(xSlice['es'].values()), dtype=float).flatten(),
                                        np.asarray(list(xSlice['es'].keys()), dtype=float).flatten(),
                                        np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float))
        li, _ = self.interp_common_wvls(np.asarray(list(xSlice['li'].values()), dtype=float).flatten(),
                                        np.asarray(list(xSlice['li'].keys()), dtype=float).flatten(),
                                        np.array(uncGrp.getDataset("LI_RADCAL_CAL").columns['1'], dtype=float))
        lt, _ = self.interp_common_wvls(np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(),
                                        np.asarray(list(xSlice['lt'].keys()), dtype=float).flatten(),
                                        np.array(uncGrp.getDataset("LT_RADCAL_CAL").columns['1'], dtype=float))

        if rhoScalar is not None:  # make rho a constant array if scalar
            rho = np.ones(len(list(esXstd.keys())))*rhoScalar
            rhoUNC, _ = self.interp_common_wvls(np.array(rhoDelta, dtype=float),
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))
        else:
            rho, _ = self.interp_common_wvls(np.array(list(rhoVec.values()), dtype=float),
                                             waveSubset,
                                             np.asarray(list(esXstd.keys()), dtype=float))
            rhoUNC, _ = self.interp_common_wvls(rhoDelta,
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_CAL")
            radcal.datasetToColumns()

            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            if ConfigFile.settings['SensorType'].lower() == "trios":
                # Convert TriOS mW/m2/nm to uW/cm^2/nm
                Coeff[sensor] = np.asarray(list(radcal.columns['2'])) / 10
            elif ConfigFile.settings['SensorType'].lower() == "seabird":
                Coeff[sensor] = np.asarray(list(radcal.columns['2']))
            Cal[sensor] = np.asarray(list(radcal.columns['3']))

            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))  # es cos stored in poldata file as temporary fix

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        Propagate_L2 = Propagate(M=100, cores=0)
        slice_size = len(es)
        ones = np.ones(slice_size)
        # zeros = np.zeros(slice_size)

        lw_means = [lt, rho, li,
                    ones, ones,
                    ones, ones,
                    ones, ones,
                    ones, ones,
                    ones, ones,
                    ones, ones]

        lw_uncertainties = [np.array(list(ltXstd.values())).flatten() * lt,
                            rhoUNC,
                            np.array(list(liXstd.values())).flatten() * li,
                            Cal['LI']/200, Cal['LT']/200,
                            cStab['LI'], cStab['LT'],
                            cLin['LI'], cLin['LT'],
                            cStray['LI']/100, cStray['LI']/100,
                            Ct['LI'], Ct['LI'],
                            cPol['LI'], cPol['LI']]

        lwAbsUnc = Propagate_L2.Propagate_Lw(lw_means, lw_uncertainties)
        lw_vals = Propagate_L2.Lw(*lw_means)

        rrs_means = [lt, rho, li, es,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones,
                     ones, ones, ones
                     ]

        rrs_uncertainties = [np.array(list(ltXstd.values())).flatten() * lt,
                             rhoUNC,
                             np.array(list(liXstd.values())).flatten() * li,
                             np.array(list(esXstd.values())).flatten() * es,
                             Cal['ES']/200, Cal['LI']/200, Cal['LT']/200,
                             cStab['ES'], cStab['LI'], cStab['LT'],
                             cLin['ES'], cLin['LI'], cLin['LT'],
                             cStray['ES']/100, cStray['LI']/100, cStray['LT']/100,
                             Ct['ES'], Ct['LI'], Ct['LT'],
                             cPol['LI'], cPol['LT'], cPol['ES']
                             ]

        rrsAbsUnc = Propagate_L2.Propagate_RRS(rrs_means, rrs_uncertainties)
        rrs_vals = Propagate_L2.RRS(*rrs_means)

        ## BAND CONVOLUTION
        # band convolution of uncertainties is done here to include uncertainty contribution of band convolution process
        Convolve = Propagate(M=100, cores=1)
        # these are absolute values! Dont get confused
        output = {}

        # interpolate output uncertainties to the waveSubset (common wavebands of interpolated es, li, & lt)
        # wvls = np.asarray(list(xSlice['es'].keys()), dtype=float)

        lwAbsUnc, _ = self.interp_common_wvls(lwAbsUnc,
                                              np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),
                                              waveSubset)
        lw_vals, _ = self.interp_common_wvls(lw_vals,
                                             np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),
                                             waveSubset)
        rrsAbsUnc, _ = self.interp_common_wvls(rrsAbsUnc,
                                               np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),
                                               waveSubset)
        rrs_vals, _ = self.interp_common_wvls(rrs_vals,
                                              np.array(uncGrp.getDataset("ES_RADCAL_CAL").columns['1'], dtype=float),
                                              waveSubset)

        ## Band Convolution of Uncertainties
        if ConfigFile.settings["bL2WeightSentinel3A"]:
            output["lwUNC_Sentinel3A"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                        [lwAbsUnc, None], "S3A")
            output["rrsUNC_Sentinel3A"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                         [rrsAbsUnc, None], "S3A")
        elif ConfigFile.settings["bL2WeightSentinel3B"]:
            output["lwUNC_Sentinel3B"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                        [lwAbsUnc, None], "S3B")
            output["rrsUNC_Sentinel3B"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                         [rrsAbsUnc, None], "S3B")
        if ConfigFile.settings['bL2WeightMODISA']:
            output["lwUNC_MODISA"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "MOD-A")
            output["rrsUNC_MODISA"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                     [rrsAbsUnc, None], "MOD-A")
        if ConfigFile.settings['bL2WeightMODIST']:
            output["lwUNC_MODIST"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "MOD-T")
            output["rrsUNC_MODIST"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                     [rrsAbsUnc, None], "MOD-T")
        if ConfigFile.settings['bL2WeightVIIRSN']:
            output["lwUNC_VIIRSN"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "VIIRS")
            output["rrsUNC_VIIRSN"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                     [rrsAbsUnc, None], "VIIRS")
        if ConfigFile.settings['bL2WeightVIIRSJ']:
            output["lwUNC_VIIRSJ"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "VIIRS")
            output["rrsUNC_VIIRSJ"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                     [rrsAbsUnc, None], "VIIRS")
            pass
        output.update({"lwUNC": lwAbsUnc, "rrsUNC": rrsAbsUnc})

        return output

    def rrsHyperUNCFACTORY(self, node, uncGrp, rhoScalar, rhoVec, rhoDelta, waveSubset, xSlice):
        """

        :param node: HDFRoot which stores L1BQC data
        :param uncGrp: HDFGroup storing the uncertainty budget
        :param rhoScalar: rho input if Mobley99 or threeC rho is used
        :param rhoVec: rho input if Zhang17 rho is used
        :param rhoDelta: uncertainties associated with rho
        :param waveSubset: wavelength subset for any band convolution (and sizing rhoScalar if used)
        :param xSlice: Dictionary of input radiance, raw_counts, standard deviations etc.

        :return: dictionary of output uncertainties that are generated
        """

        waveSubset = np.array(waveSubset, dtype=float)  # convert waveSubset to numpy array
        esXstd = xSlice['esSTD_RAW']
        liXstd = xSlice['liSTD_RAW']
        ltXstd = xSlice['ltSTD_RAW']

        es, _ = self.interp_common_wvls(np.asarray(list(xSlice['es'].values()), dtype=float).flatten(),
                                       np.asarray(list(xSlice['es'].keys()), dtype=float).flatten(),
                                       np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float))
        li, _ = self.interp_common_wvls(np.asarray(list(xSlice['li'].values()), dtype=float).flatten(),
                                       np.asarray(list(xSlice['li'].keys()), dtype=float).flatten(),
                                       np.array(uncGrp.getDataset("LI_RADCAL_UNC").columns['wvl'], dtype=float))
        lt, _ = self.interp_common_wvls(np.asarray(list(xSlice['lt'].values()), dtype=float).flatten(),
                                       np.asarray(list(xSlice['lt'].keys()), dtype=float).flatten(),
                                       np.array(uncGrp.getDataset("LT_RADCAL_UNC").columns['wvl'], dtype=float))

        if rhoScalar is not None:  # make rho a constant array if scalar
            rho = np.ones(len(list(esXstd.keys())))*rhoScalar
            rhoUNC, _ = self.interp_common_wvls(np.array(rhoDelta, dtype=float),
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))
        else:  # zhang rho needs to be interpolated to radcal wavebands (len must be 255)
            rho, _ = self.interp_common_wvls(np.array(list(rhoVec.values()), dtype=float),
                                             waveSubset,
                                             np.asarray(list(esXstd.keys()), dtype=float))
            rhoUNC, _ = self.interp_common_wvls(rhoDelta,
                                                waveSubset,
                                                np.asarray(list(esXstd.keys()), dtype=float))

        # define dictionaries for uncertainty components
        Cal = {}
        Coeff = {}
        cPol = {}
        cStray = {}
        Ct = {}
        cLin = {}
        cStab = {}

        for sensor in ["ES", "LI", "LT"]:
            radcal = uncGrp.getDataset(f"{sensor}_RADCAL_UNC")
            radcal.datasetToColumns()

            straylight = uncGrp.getDataset(f"{sensor}_STRAYDATA_CAL")
            straylight.datasetToColumns()
            cStray[sensor] = np.asarray(list(straylight.columns['1']))

            linear = uncGrp.getDataset(sensor + "_NLDATA_CAL")
            linear.datasetToColumns()
            cLin[sensor] = np.asarray(list(linear.columns['1']))

            stab = uncGrp.getDataset(sensor + "_STABDATA_CAL")
            stab.datasetToColumns()
            cStab[sensor] = np.asarray(list(stab.columns['1']))

            ### ADERU : Read coeff value from configuration files
            Cal[sensor] = np.asarray(list(radcal.columns['unc']))
            calFolder = os.path.splitext(ConfigFile.filename)[0] + "_Calibration"
            calPath = os.path.join(PATH_TO_CONFIG, calFolder)
            calibrationMap = CalibrationFileReader.read(calPath)
            waves, Coeff[sensor] = ProcessL1b_FactoryCal.extract_calibration_coeff(node, calibrationMap, sensor)

            pol = uncGrp.getDataset(sensor + "_POLDATA_CAL")
            pol.datasetToColumns()
            cPol[sensor] = np.asarray(list(pol.columns['1']))  # es cos stored in poldata file as temporary fix

            # temp uncertainties calculated at L1AQC
            Temp = uncGrp.getDataset(sensor + "_TEMPDATA_CAL")
            Temp.datasetToColumns()
            Ct[sensor] = np.array(Temp.columns[f'{sensor}_TEMPERATURE_UNCERTAINTIES'])

        Propagate_L2 = Propagate(M=100, cores=0)
        slice_size = len(es)
        ones = np.ones(slice_size)

        lw_means = [lt, rho, li,
                   ones, ones,  # Coeff['LI'], Coeff['LT'],
                   ones, ones,
                   ones, ones,
                   ones, ones,
                   ones, ones,
                   ones, ones]

        lw_uncertainties = [np.array(list(ltXstd.values())).flatten() * lt,
                           rhoUNC,
                           np.array(list(liXstd.values())).flatten() * li,
                           Cal['LI']/200, Cal['LT']/200,
                           cStab['LI'], cStab['LT'],
                           cLin['LI'], cLin['LT'],
                           cStray['LI']/100, cStray['LI']/100,
                           Ct['LI'], Ct['LI'],
                           cPol['LI'], cPol['LI']]

        # NOTE: ISSUE #95
        lwAbsUnc = Propagate_L2.Propagate_Lw(lw_means, lw_uncertainties)
        lw_vals = Propagate_L2.Lw(*lw_means)

        rrs_means = [lt, rho, li, es,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones,
                ones, ones, ones]

        rrs_uncertainties = [np.array(list(ltXstd.values())).flatten() * lt,
                             rhoUNC,
                             np.array(list(liXstd.values())).flatten() * li,
                             np.array(list(esXstd.values())).flatten() * es,
                             Cal['ES']/200, Cal['LI']/200, Cal['LT']/200,
                             cStab['ES'], cStab['LI'], cStab['LT'],
                             cLin['ES'], cLin['LI'], cLin['LT'],
                             cStray['ES']/100, cStray['LI']/100, cStray['LT']/100,
                             Ct['ES'], Ct['LI'], Ct['LT'],
                             cPol['LI'], cPol['LT'], cPol['ES']
                             ]

        rrsAbsUnc = Propagate_L2.Propagate_RRS(rrs_means, rrs_uncertainties)
        rrs_vals = Propagate_L2.RRS(*rrs_means)

        ## BAND CONVOLUTION
        # band convolution of uncertainties is done here to include uncertainty contribution of band convolution process
        Convolve = Propagate(M=100, cores=1)
        # these are absolute values! Dont get confused

        output = {}  # create dictionary to store uncertainty values which are returned from methods

        # interpolate output uncertainties to the waveSubset (common wavebands of interpolated es, li, & lt)
        # wvls = np.asarray(list(xSlice['es'].keys()), dtype=float)
        lwAbsUnc, _ = self.interp_common_wvls(lwAbsUnc,
                                             np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                             waveSubset)
        lw_vals, _ = self.interp_common_wvls(lw_vals,
                                            np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                            waveSubset)
        rrsAbsUnc, _ = self.interp_common_wvls(rrsAbsUnc,
                                              np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                              waveSubset)
        rrs_vals, _ = self.interp_common_wvls(rrs_vals,
                                             np.array(uncGrp.getDataset("ES_RADCAL_UNC").columns['wvl'], dtype=float),
                                             waveSubset)

        if ConfigFile.settings["bL2WeightSentinel3A"]:
            output["lwUNC_Sentinel3A"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                        [lwAbsUnc, None], "S3A")
            output["rrsUNC_Sentinel3A"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                         [rrsAbsUnc, None], "S3A")
        elif ConfigFile.settings["bL2WeightSentinel3B"]:
            output["lwUNC_Sentinel3B"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                        [lwAbsUnc, None], "S3B")
            output["rrsUNC_Sentinel3B"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                         [rrsAbsUnc, None], "S3B")
        if ConfigFile.settings['bL2WeightMODISA']:
            output["lwUNC_MODISA"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "MOD-A")
            output["rrsUNC_MODISA"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                     [rrsAbsUnc, None], "MOD-A")
        if ConfigFile.settings['bL2WeightMODIST']:
            output["lwUNC_MODIST"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "MOD-T")
            output["rrsUNC_MODIST"] = Convolve.band_Conv_Uncertainty([rrs_vals,waveSubset],
                                                                     [rrsAbsUnc, None], "MOD-T")
        if ConfigFile.settings['bL2WeightVIIRSN']:
            output["lwUNC_VIIRSN"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                    [lwAbsUnc, None], "VIIRS")
            output["rrsUNC_VIIRSN"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                     [rrsAbsUnc, None], "VIIRS")
        if ConfigFile.settings['bL2WeightVIIRSJ']:
            output["lwUNC_VIIRSJ"] = Convolve.band_Conv_Uncertainty([lw_vals, waveSubset],
                                                                   [lwAbsUnc, None], "VIIRS")
            output["rrsUNC_VIIRSJ"] = Convolve.band_Conv_Uncertainty([rrs_vals, waveSubset],
                                                                    [rrsAbsUnc, None], "VIIRS")
            pass
        output.update({"lwUNC": lwAbsUnc, "rrsUNC": rrsAbsUnc})

        return output

    ## Utilties
    @staticmethod
    def interp_common_wvls(columns, waves, newWavebands):
        saveTimetag2 = None
        if isinstance(columns, dict):
            if "Datetag" in columns:
                saveDatetag = columns.pop("Datetag")
                saveTimetag2 = columns.pop("Timetag2")
                columns.pop("Datetime")
            y = np.asarray(list(columns.values()))
        elif isinstance(columns, np.ndarray):  # is numpy array
            y = columns
        else:
            msg = "columns are unexpected type: ProcessInstrumentUncertainties.py - interp_common_wvls"
            print(msg)
        # Get wavelength values
        x = np.asarray(waves)

        newColumns = collections.OrderedDict()
        if saveTimetag2 is not None:
            newColumns["Datetag"] = saveDatetag
            newColumns["Timetag2"] = saveTimetag2
        # Can leave Datetime off at this point

        for i in range(newWavebands.shape[0]):
            newColumns[str(round(10*newWavebands[i])/10)] = []  # limit to one decimal place

        new_y = np.interp(newWavebands, x, y)  #InterpolatedUnivariateSpline(x, y, k=3)(newWavebands)

        for waveIndex in range(newWavebands.shape[0]):
            newColumns[str(round(10*newWavebands[waveIndex])/10)].append(new_y[waveIndex])

        return new_y, newColumns

    @staticmethod
    def interpolateSamples(Columns, waves, newWavebands):
        '''
        Wavelength Interpolation for differently sized arrays containing samples
        Use a common waveband set determined by the maximum lowest wavelength
        of all sensors, the minimum highest wavelength, and the interval
        set in the Configuration Window.
        '''

        # Copy dataset to dictionary
        columns = {k: Columns[:, i] for i, k in enumerate(waves)}
        cols = []
        for m in range(Columns.shape[0]):  # across all the monte carlo draws
            newColumns = {}

            for i in range(newWavebands.shape[0]):
                # limit to one decimal place
                newColumns[str(round(10*newWavebands[i])/10)] = []

            # for m in range(Columns.shape[0]):
            # Perform interpolation for each timestamp
            y = np.asarray([columns[k][m] for k in columns])

            new_y = sp.interpolate.InterpolatedUnivariateSpline(waves, y, k=3)(newWavebands)

            for waveIndex in range(newWavebands.shape[0]):
                newColumns[str(round(10*newWavebands[waveIndex])/10)].append(new_y[waveIndex])

            cols.append(newColumns)

        return np.asarray(cols)

    # Measurement Functions
    @staticmethod
    def S12func(k, S1, S2):
        return ((1 + k)*S1) - (k*S2)

    @staticmethod
    def alphafunc(S1, S12):
        t1 = [Decimal(S1[i]) - Decimal(S12[i]) for i in range(len(S1))]
        t2 = [pow(Decimal(S12[i]), 2) for i in range(len(S12))]
        return np.asarray([float(t1[i]/t2[i]) for i in range(len(t1))])

    @staticmethod
    def dark_Substitution(light, dark):
        return light - dark

    @staticmethod
    def non_linearity_corr(offset_corrected_mesure, alpha):
        linear_corr_mesure = offset_corrected_mesure*(1 - alpha*offset_corrected_mesure)
        return linear_corr_mesure

    @staticmethod
    def Slaper_SL_correction(input_data, SL_matrix, n_iter=5):
        nband = len(input_data)
        m_norm = np.zeros(nband)

        mC = np.zeros((n_iter + 1, nband))
        mX = np.zeros((n_iter + 1, nband))
        mZ = SL_matrix
        mX[0, :] = input_data

        for i in range(nband):
            jstart = np.max([0, i - 10])
            jstop = np.min([nband, i + 10])
            m_norm[i] = np.sum(mZ[i, jstart:jstop])  # eq 4

        for i in range(nband):
            if m_norm[i] == 0:
                mZ[i, :] = np.zeros(nband)
            else:
                mZ[i, :] = mZ[i, :]/m_norm[i]  # eq 5

        for k in range(1, n_iter + 1):
            for i in range(nband):
                mC[k - 1, i] = mC[k - 1, i] + np.sum(mX[k - 1, :]*mZ[i, :])  # eq 6
                if mC[k - 1, i] == 0:
                    mX[k, i] = 0
                else:
                    mX[k, i] = (mX[k - 1, i]*mX[0, i])/mC[k - 1, i]  # eq 7

        return mX[n_iter - 1, :]

    @staticmethod
    def absolute_calibration(normalized_mesure, updated_radcal_gain):
        return normalized_mesure/updated_radcal_gain

    @staticmethod
    def thermal_corr(Ct, calibrated_mesure):
        return Ct*calibrated_mesure

    @staticmethod
    def prepare_cos(uncGrp, sensortype, level=None):
        """
        read from hdf and prepare inputs for cos_err measurement function
        """
        ## Angular cosine correction (for Irradiance)
        if level != 'L2':
            radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
            coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:, 2:]
            cos_unc = (np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:, 2:]
                       /100)*coserror

            coserror_90 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
            cos90_unc = (np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:,
                         2:]/100)*coserror_90
        else:
            # reading in data changes if at L2 (because hdf files have different layout)
            radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
            coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:, 2:]
            cos_unc = (np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:, 2:]/100)*coserror
            coserror_90 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
            cos90_unc = (np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:, 2:]/100)*coserror_90

        radcal_unc = None  # no uncertainty in the wavelengths as they are only used to index

        zenith_ang = uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").attributes["COLUMN_NAMES"].split('\t')[2:]
        zenith_ang = np.asarray([float(x) for x in zenith_ang])
        zen_unc = np.asarray([0.05 for x in zenith_ang])  # default of 0.5 for solar zenith unc

        return [radcal_wvl, coserror, coserror_90, zenith_ang], [radcal_unc, cos_unc, cos90_unc, zen_unc]

    @staticmethod
    def AZAvg_Coserr(coserror, coserror_90):
        # if delta < 2% : averaging the 2 azimuth plan
        return (coserror + coserror_90)/2.  # average azi coserr

    @staticmethod
    def ZENAvg_Coserr(radcal_wvl, AZI_avg_coserror):
        i1 = np.argmin(np.abs(radcal_wvl - 300))
        i2 = np.argmin(np.abs(radcal_wvl - 1000))

        # if delta < 2% : averaging symetric zenith
        ZEN_avg_coserror = (AZI_avg_coserror + AZI_avg_coserror[:, ::-1])/2.

        # set coserror to 1 outside range [450,700]
        ZEN_avg_coserror[0:i1, :] = 0
        ZEN_avg_coserror[i2:, :] = 0
        return ZEN_avg_coserror

    @staticmethod
    def FHemi_Coserr(ZEN_avg_coserror, zenith_ang):
        # Compute full hemisperical coserror
        zen0 = np.argmin(np.abs(zenith_ang))
        zen90 = np.argmin(np.abs(zenith_ang - 90))
        deltaZen = (zenith_ang[1::] - zenith_ang[:-1])

        full_hemi_coserror = np.zeros(255)

        for i in range(255):
            full_hemi_coserror[i] = np.sum(
                ZEN_avg_coserror[i, zen0:zen90]*np.sin(2*np.pi*zenith_ang[zen0:zen90]/180)*deltaZen[
                                                                                           zen0:zen90]*np.pi/180)

        return full_hemi_coserror

    @staticmethod
    def cosine_corr(avg_coserror, full_hemi_coserror, zenith_ang, thermal_corr_mesure, sol_zen, dir_rat):
        ind_closest_zen = np.argmin(np.abs(zenith_ang - sol_zen))
        cos_corr = 1 - avg_coserror[:, ind_closest_zen]/100
        Fhcorr = 1 - np.array(full_hemi_coserror)/100
        cos_corr_mesure = (dir_rat*thermal_corr_mesure*cos_corr) + ((1 - dir_rat)*thermal_corr_mesure*Fhcorr)

        return cos_corr_mesure

    @staticmethod
    def cos_corr_fun(avg_coserror, zenith_ang, sol_zen):
        ind_closest_zen = np.argmin(np.abs(zenith_ang - sol_zen))
        return 1 - avg_coserror[:, ind_closest_zen]/100

    @staticmethod
    def cosine_error_correction(uncGrp, sensortype):

        ## Angular cosine correction (for Irradiance)
        radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())
        coserror = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").data))[1:,2:]
        coserror_90 = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR_AZ90").data))[1:, 2:]
        coserror_unc = (np.asarray(
            pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY").data))[1:,2:]/100)*coserror
        coserror_90_unc = (np.asarray(
            pd.DataFrame(uncGrp.getDataset(sensortype + "_ANGDATA_UNCERTAINTY_AZ90").data))[1:, 2:]/100)*coserror_90
        zenith_ang = uncGrp.getDataset(sensortype + "_ANGDATA_COSERROR").attributes["COLUMN_NAMES"].split('\t')[2:]
        i1 = np.argmin(np.abs(radcal_wvl - 300))
        i2 = np.argmin(np.abs(radcal_wvl - 1000))
        zenith_ang = np.asarray([float(x) for x in zenith_ang])

        # comparing cos_error for 2 azimuth
        AZI_delta_err = np.abs(coserror - coserror_90)

        # if delta < 2% : averaging the 2 azimuth plan
        AZI_avg_coserror = (coserror + coserror_90)/2.
        AZI_delta = np.power(np.power(coserror_unc, 2) + np.power(coserror_90_unc, 2), 0.5)  # TODO: check this!

        # comparing cos_error for symetric zenith
        ZEN_delta_err = np.abs(AZI_avg_coserror - AZI_avg_coserror[:, ::-1])
        ZEN_delta = np.power(np.power(AZI_delta, 2) + np.power(AZI_delta[:, ::-1], 2), 0.5)

        # if delta < 2% : averaging symetric zenith
        ZEN_avg_coserror = (AZI_avg_coserror + AZI_avg_coserror[:, ::-1])/2.

        # set coserror to 1 outside range [450,700]
        ZEN_avg_coserror[0:i1, :] = 0
        ZEN_avg_coserror[i2:, :] = 0

        return ZEN_avg_coserror, AZI_avg_coserror, zenith_ang, ZEN_delta_err, ZEN_delta, AZI_delta_err, AZI_delta


class HyperOCR(Instrument):
    def __init__(self):
        super().__init__()
        self.instrument = "HyperOCR"

    @staticmethod
    def _check_data(dark, light):
        msg = None
        if (dark is None) or (light is None):
            msg = f'Dark Correction, dataset not found: {dark} , {light}'
            print(msg)
            Utilities.writeLogFile(msg)
            return False

        if Utilities.hasNan(light):
            frameinfo = getframeinfo(currentframe())
            msg = f'found NaN {frameinfo.lineno}'

        if Utilities.hasNan(dark):
            frameinfo = getframeinfo(currentframe())
            msg = f'found NaN {frameinfo.lineno}'
        if msg:
            print(msg)
            Utilities.writeLogFile(msg)
        return True

    def darkToLightTimer(self, rawGrp, sensortype):
        darkGrp = rawGrp['DARK']
        lightGrp = rawGrp['LIGHT']

        if darkGrp.attributes["FrameType"] == "ShutterDark" and darkGrp.getDataset(sensortype):
            darkData = darkGrp.getDataset(sensortype)
            darkDateTime = darkGrp.getDataset("DATETIME")
        if lightGrp.attributes["FrameType"] == "ShutterLight" and lightGrp.getDataset(sensortype):
            lightData = lightGrp.getDataset(sensortype)
            lightDateTime = lightGrp.getDataset("DATETIME")

        if darkGrp is None or lightGrp is None:
            msg = f'No radiometry found for {sensortype}'
            print(msg)
            Utilities.writeLogFile(msg)
            return False
        elif not self._check_data(darkData, lightData):
            return False

        newDarkData = self._interp(lightData, lightDateTime, darkData, darkDateTime)
        if isinstance(newDarkData, bool):
            return False
        else:
            rawGrp['DARK'].datasets[sensortype].data = newDarkData
            rawGrp['DARK'].datasets[sensortype].datasetToColumns()
            return True

    @staticmethod
    def _interp(lightData, lightTimer, darkData, darkTimer):
        # Interpolate Dark Dataset to match number of elements as Light Dataset
        newDarkData = np.copy(lightData.data)
        for k in darkData.data.dtype.fields.keys():  # darkData.data.dtype.fields.keys():  # For each wavelength
            x = np.copy(darkTimer.data).tolist()  # darktimer
            y = np.copy(darkData.data[k]).tolist()  # data at that band over time
            new_x = lightTimer.data  # lighttimer

            if len(x) < 3 or len(y) < 3 or len(new_x) < 3:
                msg = "**************Cannot do cubic spline interpolation, length of datasets < 3"
                print(msg)
                Utilities.writeLogFile(msg)
                return False
            if not Utilities.isIncreasing(x):
                msg = "**************darkTimer does not contain strictly increasing values"
                print(msg)
                Utilities.writeLogFile(msg)
                return False
            if not Utilities.isIncreasing(new_x):
                msg = "**************lightTimer does not contain strictly increasing values"
                print(msg)
                Utilities.writeLogFile(msg)
                return False

            if len(x) >= 3:
                # Because x is now a list of datetime tuples, they'll need to be
                # converted to Unix timestamp values
                xTS = [calendar.timegm(xDT.utctimetuple()) + xDT.microsecond / 1E6 for xDT in x]
                newXTS = [calendar.timegm(xDT.utctimetuple()) + xDT.microsecond / 1E6 for xDT in new_x]

                newDarkData[k] = Utilities.interp(xTS,y,newXTS, fill_value=np.nan)

                for val in newDarkData[k]:
                    if np.isnan(val):
                        frameinfo = getframeinfo(currentframe())
                        msg = f'found NaN {frameinfo.lineno}'
            else:
                msg = '**************Record too small for splining. Exiting.'
                print(msg)
                Utilities.writeLogFile(msg)
                return False

        if Utilities.hasNan(darkData):
            frameinfo = getframeinfo(currentframe())
            msg = f'found NaN {frameinfo.lineno}'
            print(msg)
            Utilities.writeLogFile(msg)
            exit()

        return newDarkData

    def lightDarkStats(self, grp, slice, sensortype):
        # SeaBird HyperOCR
        lightGrp = grp[0]
        lightSlice = slice[0]
        darkGrp = grp[1]
        darkSlice = slice[1]

        if darkGrp.attributes["FrameType"] == "ShutterDark" and darkGrp.getDataset(sensortype):
            darkData = darkSlice['data']  # darkGrp.getDataset(sensortype)
            # darkDateTime = darkSlice['datetime']  # darkGrp.getDataset("DATETIME")

        if lightGrp.attributes["FrameType"] == "ShutterLight" and lightGrp.getDataset(sensortype):
            lightData = lightSlice['data']  # lightGrp.getDataset(sensortype)
            # lightDateTime = lightSlice['datetime']  # lightGrp.getDataset("DATETIME")

        if darkGrp is None or lightGrp is None:
            msg = f'No radiometry found for {sensortype}'
            print(msg)
            Utilities.writeLogFile(msg)
            return False

        elif not self._check_data(darkData, lightData):
            return False
        # Do interpolation at the start of the stations ensemble process, then slice like light data
        # newDarkData = self._interp(lightData, lightDateTime, darkData, darkDateTime)
        # if not newDarkData:
        #     return False

        # Correct light data by subtracting interpolated dark data from light data
        std_Light = []
        std_Dark = []
        ave_Light = []
        ave_Dark = []
        stdevSignal = {}

        # number of replicates for light and dark readings
        N = np.asarray(list(lightData.values())).shape[1]
        Nd = np.asarray(list(darkData.values())).shape[1]

        for i, k in enumerate(lightData.keys()):
            wvl = str(float(k))

            # apply normalisation to the standard deviations used in uncertainty calculations
            std_Light.append(np.std(lightData[k])/pow(N, 0.5))  # = (sigma / sqrt(N))**2 or sigma**2
            std_Dark.append(np.std(darkData[k])/pow(Nd, 0.5))  # sigma here is essentially sigma**2 so N must be rooted
            ave_Light.append(np.average(lightData[k]))
            ave_Dark.append(np.average(darkData[k]))

            for x in range(N):
                lightData[k][x] -= darkData[k][x]

            signalAve = np.average(lightData[k])

            # Normalised signal standard deviation =
            if signalAve:
                stdevSignal[wvl] = pow((pow(std_Light[i], 2) + pow(std_Dark[i], 2))/pow(signalAve, 2), 0.5)
            else:
                stdevSignal[wvl] = 0.0

        return dict(
            ave_Light=np.array(ave_Light),
            ave_Dark=np.array(ave_Dark),
            std_Light=np.array(std_Light),
            std_Dark=np.array(std_Dark),
            std_Signal=stdevSignal,
            )

    def FRM(self, node, uncGrp, raw_grps, raw_slices, stats, newWaveBands, refData: dict):
        # calibration of HyperOCR following the FRM processing of FRM4SOC2
        output = {}
        for sensortype in ['ES', 'LI', 'LT']:
            print('FRM Processing:', sensortype)

            raw_name = node.attributes['RAW_FILE_NAME']
            cast = raw_name[raw_name.find('FICE22'):raw_name.find('.mlb')]
            refGrp = np.asarray(list(refData[sensortype].values()), dtype=float).flatten()
            fig = plt.figure(f"{sensortype}_{os.path.basename(cast)}")

            # Read data
            grp = raw_grps[sensortype]
            raw_data = np.asarray(list(raw_slices[sensortype]["LIGHT"]['data'].values())).transpose()
            raw_dark = np.asarray(list(raw_slices[sensortype]["DARK"]['data'].values())).transpose()

            # read in data for FRM processing
            # raw_data = np.asarray(list(slice.values())).transpose()  # raw_data = np.asarray(grp.getDataset(sensortype).data.tolist())  # dark subtracted signal
            # raw_data = np.asarray(list(slice['data'].values())).transpose()
            # raw_data = np.asarray(grp.getDataset(sensortype).data.tolist())  # dark subtracted signal
            int_time = np.asarray(grp.getDataset("INTTIME").data.tolist())
            int_time = np.mean(int_time)

            # Read FRM characterisation
            radcal_wvl = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())

            refGrp, _ = self.interp_common_wvls(refGrp,
                                                np.asarray(list(refData[sensortype].keys()), dtype=float),
                                                radcal_wvl)

            radcal_cal = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['2']
            S1 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['6']
            S2 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['8']
            # TODO: Check if multiplying by np.abs(S1/S2) is correct
            S1_unc = (pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['7']/100)[1:].to_list()*np.abs(S1[1:])
            S2_unc = (pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['9']/100)[1:].to_list()*np.abs(S2[1:])
            mZ = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_LSF").data))
            mZ_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_UNCERTAINTY").data))

            # remove 1st line and column, we work on 255 pixel not 256.
            mZ = mZ[1:, 1:]
            mZ_unc = mZ_unc[1:, 1:]

            Ct = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data
                                         )[f'{sensortype}_TEMPERATURE_COEFFICIENTS'][1:].tolist())
            Ct_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data
                                             )[f'{sensortype}_TEMPERATURE_UNCERTAINTIES'][1:].tolist())
            LAMP = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['2'])
            LAMP_unc = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['3'])/100*LAMP

            # Defined constants
            nband = len(radcal_wvl)
            n_iter = 5

            # set up uncertainty propagation
            mDraws = 100  # number of monte carlo draws
            prop = punpy.MCPropagation(mDraws, parallel_cores=1)

            # uncertainties from data:
            sample_int_time = cm.generate_sample(mDraws, int_time, None, None)
            sample_n_iter = cm.generate_sample(mDraws, n_iter, None, None, dtype=int)
            sample_mZ = cm.generate_sample(mDraws, mZ, mZ_unc, "rand")
            sample_Ct = cm.generate_sample(mDraws, Ct, Ct_unc, "syst")

            # pad Lamp data and generate sample
            LAMP = np.pad(LAMP, (0, nband - len(LAMP)), mode='constant')  # PAD with zero if not 255 long
            LAMP_unc = np.pad(LAMP_unc, (0, nband - len(LAMP_unc)), mode='constant')
            sample_LAMP = cm.generate_sample(mDraws, LAMP, LAMP_unc, "syst")

            # Non-linearity alpha computation
            cal_int = radcal_cal.pop(0)
            sample_cal_int = cm.generate_sample(100, cal_int, None, None)

            t1 = S1.iloc[0]
            S1 = S1.drop(S1.index[0])
            t2 = S2.iloc[0]
            S2 = S2.drop(S2.index[0])

            S1 = np.asarray(S1, dtype=float)
            S2 = np.asarray(S2, dtype=float)

            sample_t1 = cm.generate_sample(mDraws, t1, None, None)
            sample_S1 = cm.generate_sample(mDraws, np.asarray(S1), S1_unc, "rand")
            sample_S2 = cm.generate_sample(mDraws, np.asarray(S2), S2_unc, "rand")

            k = t1/(t2 - t1)
            sample_k = cm.generate_sample(mDraws, k, None, None)
            S12 = self.S12func(k, S1, S2)
            sample_S12 = prop.run_samples(self.S12func, [sample_k, sample_S1, sample_S2])

            S12_sl_corr = self.Slaper_SL_correction(S12, mZ, n_iter=5)
            S12_sl_corr_unc = []
            sl4 = self.Slaper_SL_correction(S12, mZ, n_iter=4)
            for i in range(len(S12_sl_corr)):  # get the difference between n=4 and n=5
                if S12_sl_corr[i] > sl4[i]:
                    S12_sl_corr_unc.append(S12_sl_corr[i] - sl4[i])
                else:
                    S12_sl_corr_unc.append(sl4[i] - S12_sl_corr[i])

            sample_S12_sl_syst = cm.generate_sample(mDraws, S12_sl_corr, np.array(S12_sl_corr_unc), "syst")
            sample_S12_sl_rand = prop.run_samples(self.Slaper_SL_correction, [sample_S12, sample_mZ, sample_n_iter])
            sample_S12_sl_corr = prop.combine_samples([sample_S12_sl_syst, sample_S12_sl_rand])

            # alpha = ((S1-S12)/(S12**2)).tolist()
            alpha = self.alphafunc(S1, S12)
            sample_alpha = prop.run_samples(self.alphafunc, [sample_S1, sample_S12])
            # alpha_unc = np.power(np.power(S1_unc, 2) + np.power(S12_unc, 2) + np.power(S2_unc, 2), 0.5)  # TODO: change this!
            # sample_alpha = cm.generate_sample(mDraws, alpha, alpha_unc, "syst")

            # Updated calibration gain
            if sensortype == "ES":
                ## Compute avg cosine error
                cos_mean_vals, cos_uncertainties = self.prepare_cos(uncGrp, sensortype, 'L2')
                corr = [None, "syst", "syst", "rand"]
                sample_radcal_wvl, sample_coserr, sample_coserr90, sample_zen_ang = [
                    cm.generate_sample(mDraws, samp, cos_uncertainties[i], corr[i]) for i, samp in
                    enumerate(cos_mean_vals)]

                avg_coserror, avg_azi_coserror, zenith_ang, zen_delta, azi_delta, zen_unc, azi_unc = \
                    self.cosine_error_correction(uncGrp, sensortype)

                # error due to lack of symmetry in cosine response
                sample_azi_delta_err1 = cm.generate_sample(mDraws, avg_azi_coserror, azi_unc, "syst")
                sample_azi_delta_err2 = cm.generate_sample(mDraws, avg_azi_coserror, azi_delta, "syst")
                sample_azi_delta_err = prop.combine_samples([sample_azi_delta_err1, sample_azi_delta_err2])
                sample_azi_err = prop.run_samples(self.AZAvg_Coserr, [sample_coserr, sample_coserr90])
                sample_azi_avg_coserror = prop.combine_samples([sample_azi_err, sample_azi_delta_err])

                sample_zen_delta_err1 = cm.generate_sample(mDraws, avg_coserror, zen_unc, "syst")
                sample_zen_delta_err2 = cm.generate_sample(mDraws, avg_coserror, zen_delta, "syst")
                sample_zen_delta_err = prop.combine_samples([sample_zen_delta_err1, sample_zen_delta_err2])
                sample_zen_err = prop.run_samples(self.ZENAvg_Coserr, [sample_radcal_wvl, sample_azi_avg_coserror])
                sample_zen_avg_coserror = prop.combine_samples([sample_zen_err, sample_zen_delta_err])

                # full_hemi_coserr = self.FHemi_Coserr(avg_coserror, zenith_ang)
                sample_fhemi_coserr = prop.run_samples(self.FHemi_Coserr, [sample_zen_avg_coserror, sample_zen_ang])

                ## Irradiance direct and diffuse ratio
                # res_py6s = ProcessL1b_FRMCal.get_direct_irradiance_ratio(node, sensortype, trios=0)
                res_py6s = ProcessL1b_FRMCal.get_direct_irradiance_ratio(node, sensortype, called_L2=True)

                updated_radcal_gain = self.update_cal_ES(S12_sl_corr, LAMP, cal_int, t1)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_ES,
                                                              [sample_S12_sl_corr, sample_LAMP, sample_cal_int,
                                                               sample_t1])
            else:
                PANEL = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['2'])
                PANEL_unc = (np.asarray(
                    pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['3'])/100)*PANEL
                PANEL = np.pad(PANEL, (0, nband - len(PANEL)), mode='constant')
                PANEL_unc = np.pad(PANEL_unc, (0, nband - len(PANEL_unc)), mode='constant')
                sample_PANEL = cm.generate_sample(100, PANEL, PANEL_unc, "syst")
                updated_radcal_gain = self.update_cal_rad(S12_sl_corr, LAMP, PANEL, cal_int, t1)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_rad,
                                                              [sample_S12_sl_corr, sample_LAMP, sample_PANEL,
                                                               sample_cal_int,
                                                               sample_t1])

            ## sensitivity factor : if gain==0 (or NaN), no calibration is performed and data is affected to 0
            ind_zero = radcal_cal <= 0
            ind_nan = np.isnan(radcal_cal)
            ind_nocal = ind_nan | ind_zero
            # set 1 instead of 0 to perform calibration (otherwise division per 0)
            updated_radcal_gain[ind_nocal == True] = 1

            alpha = np.asarray(alpha)
            # Ct = np.asarray(Ct)

            # Filter Raw Data
            # ind_raw_data = (radcal_cal[radcal_wvl > 0]) > 0
            # raw_filtered = np.asarray([raw_data[n][ind_raw_data] for n in range(nmes)])
            # dark_filtered = np.asarray([raw_dark[n][ind_raw_data] for n in range(nmes)])
            data = np.mean(raw_data, axis=0)  # raw data already dark subtracted, use mean for statistical analysis

            # signal uncertainties
            std_light = stats[sensortype]['std_Light']  # standard deviations are taken from generateSensorStats
            std_dark = stats[sensortype]['std_Dark']
            sample_light = cm.generate_sample(100, np.mean(raw_data, axis=0), std_light, "rand")
            sample_dark = cm.generate_sample(100, np.mean(raw_dark, axis=0), std_dark, "rand")
            sample_dark_corr_data = prop.run_samples(self.dark_Substitution, [sample_light, sample_dark])

            dark_unc = prop.process_samples(None, sample_dark_corr_data)
            dark_unc_rel = np.abs((dark_unc * 1e10) / (refGrp * 1e10)) * 100
            plt.plot(radcal_wvl, dark_unc_rel, label="+Noise")
            light_unc = prop.process_samples(None, sample_light)
            std_rel = np.abs((light_unc * 1e10) / (refGrp * 1e10)) * 100
            plt.plot(radcal_wvl, std_rel, label="Environmental Instability")

            # Non-linearity
            data1 = self.DATA1(data, alpha)  # data*(1 - alpha*data)
            sample_data1 = prop.run_samples(self.DATA1, [sample_dark_corr_data, sample_alpha])

            calFolder = os.path.splitext(ConfigFile.filename)[0] + "_Calibration"
            calPath = os.path.join(PATH_TO_CONFIG, calFolder)
            calibrationMap = CalibrationFileReader.read(calPath)
            waves, Coef = ProcessL1b_FactoryCal.extract_calibration_coeff(node, calibrationMap, sensortype)
            nlin_unc = prop.process_samples(None, sample_data1)
            nlin_unc_rel = np.abs((nlin_unc * Coef * 1e10)/(refGrp * 1e10)) * 100
            plt.plot(radcal_wvl, nlin_unc_rel, label="+nLin")

            # Straylight
            data2 = self.Slaper_SL_correction(data1, mZ, n_iter)

            S12_sl_corr_unc = []
            sl4 = self.Slaper_SL_correction(data1, mZ, n_iter=4)
            for i in range(len(data2)):  # get the difference between n=4 and n=5
                if data1[i] > sl4[i]:
                    S12_sl_corr_unc.append(data2[i] - sl4[i])
                else:
                    S12_sl_corr_unc.append(sl4[i] - data2[i])

            sample_straylight_1 = cm.generate_sample(mDraws, data2, np.array(S12_sl_corr_unc), "syst")  # model error of method
            sample_straylight_2 = prop.run_samples(self.Slaper_SL_correction,
                                                   [sample_data1, sample_mZ, sample_n_iter])  # error from method
            sample_data2 = prop.combine_samples([sample_straylight_1, sample_straylight_2])  # total straylight uncertainty



            norm_unc = prop.process_samples(None, sample_data2)
            rel_norm_unc = (norm_unc * 1e10 / refGrp * 1e10) * 100  # normalized_mesure
            plt.plot(radcal_wvl, rel_norm_unc, label="+Straylight")

            # Calibration
            data3 = self.DATA3(data2, cal_int, int_time, updated_radcal_gain)  # data2*(cal_int/int_time)/updated_radcal_gain
            sample_data3 = prop.run_samples(self.DATA3, [sample_data2, sample_cal_int, sample_int_time, sample_updated_radcal_gain])

            # thermal
            data4 = self.DATA4(data3, Ct)
            sample_data4 = prop.run_samples(self.DATA4, [sample_data3, sample_Ct])
            therm_unc = prop.process_samples(None, sample_data4)
            therm_unc_rel = np.abs((therm_unc * 1e10) / (data4 * 1e10)) * 100  # refGrp
            plt.plot(radcal_wvl, therm_unc_rel, label="+Thermal")

            # Cosine correction
            if sensortype == "ES":
                solar_zenith = np.array(res_py6s['solar_zenith'])
                direct_ratio = res_py6s['direct_ratio']

                sample_sol_zen = cm.generate_sample(mDraws, solar_zenith,
                                                    np.asarray([0.05 for i in range(np.size(solar_zenith))]),
                                                    "rand")  # TODO: get second opinion on zen unc in 6S

                sample_dir_rat = cm.generate_sample(mDraws, direct_ratio, 0.08*direct_ratio, "syst")

                data5 = self.DATA5(data4, solar_zenith, direct_ratio, zenith_ang, avg_coserror, full_hemi_coserr)
                sample_data5 = prop.run_samples(self.DATA5, [sample_data4,
                                                             sample_sol_zen,
                                                             sample_dir_rat,
                                                             sample_zen_ang,
                                                             sample_zen_avg_coserror, # check that zen_avg_coserror is correct
                                                             sample_fhemi_coserr])
                unc = prop.process_samples(None, sample_data5)
                sample = sample_data5
                cos_unc_rel = np.abs((unc * 1e10)/(data5 * 1e10))*100  # unc in % - refGrp
                plt.plot(radcal_wvl, cos_unc_rel, label="+Cosine")
            else:
                unc = prop.process_samples(None, sample_data4)
                sample = sample_data4

            # setup plot settings and save
            plt.xlim((350, 900))
            plt.ylim((0, 5))
            plt.title(f"{os.path.basename(cast)}: {sensortype} - Breakdown FRM")
            plt.legend()
            plt.savefig(f"{sensortype}_{os.path.basename(cast)}_breakdown.jpg")

            output[f"{sensortype.lower()}Wvls"] = radcal_wvl[ind_nocal == False]
            output[f"{sensortype.lower()}Unc"] = unc[ind_nocal == False]  # relative uncertainty
            output[f"{sensortype.lower()}Sample"] = sample[:, ind_nocal == False]  # samples keep raw

            # sort the outputs ready for following process
            # get sensor specific wavebands to be keys for uncs, then remove from output
            wvls = np.asarray(output.pop(f"{sensortype.lower()}Wvls"), dtype=float)
            _, output[f"{sensortype.lower()}Unc"] = self.interp_common_wvls(
                output[f"{sensortype.lower()}Unc"], wvls, newWaveBands)
            output[f"{sensortype.lower()}Sample"] = self.interpolateSamples(
                output[f"{sensortype.lower()}Sample"], wvls, newWaveBands)

        return output

    # Measurement Functions
    @staticmethod
    def DATA1(data, alpha):
        return data*(1 - alpha*data)

    @staticmethod
    def DATA3(data2, cal_int, int_time, updated_radcal_gain):
        return data2*(cal_int/int_time)/updated_radcal_gain

    @staticmethod
    def DATA4(data3, Ct):
        data4 = data3*Ct
        data4[data4 <= 0] = 0
        return data4

    @staticmethod
    def DATA5(data4, solar_zenith, direct_ratio, zenith_ang, avg_coserror, full_hemi_coserror):
        ind_closest_zen = np.argmin(np.abs(zenith_ang - solar_zenith))
        cos_corr = (1 - avg_coserror[:, ind_closest_zen]/100)
        Fhcorr = (1 - full_hemi_coserror/100)
        return (direct_ratio*data4*cos_corr) + ((1 - direct_ratio)*data4*Fhcorr)

    @staticmethod
    def update_cal_ES(S12_sl_corr, LAMP, cal_int, t1):
        return (S12_sl_corr/LAMP)*(10*cal_int/t1)

    @staticmethod
    def update_cal_rad(S12_sl_corr, LAMP, PANEL, cal_int, t1):
        return (np.pi*S12_sl_corr)/(LAMP*PANEL)*(10*cal_int/t1)


class Trios(Instrument):
    def __init__(self):
        super().__init__()

    def lightDarkStats(self, grp, slice, sensortype):
        raw_cal = grp.getDataset(f"CAL_{sensortype}").data
        raw_back = np.asarray(grp.getDataset("BACK_"+sensortype).data.tolist())
        raw_data = np.asarray(list(slice['data'].values())).transpose()  # data is transpose of old version

        raw_wvl = np.array(pd.DataFrame(grp.getDataset(sensortype).data).columns)
        int_time = np.asarray(grp.getDataset("INTTIME").data.tolist())
        DarkPixelStart = int(grp.attributes["DarkPixelStart"])
        DarkPixelStop = int(grp.attributes["DarkPixelStop"])
        int_time_t0 = int(grp.getDataset(f"BACK_{sensortype}").attributes["IntegrationTime"])

        # sensitivity factor : if raw_cal==0 (or NaN), no calibration is performed and data is affected to 0
        ind_zero = np.array([rc[0] == 0 for rc in raw_cal])  # changed due to raw_cal now being a np array
        ind_nan = np.array([np.isnan(rc[0]) for rc in raw_cal])
        ind_nocal = ind_nan | ind_zero
        raw_cal = np.array([rc[0] for rc in raw_cal])
        raw_cal[ind_nocal==True] = 1

        # check size of data
        nband = len(raw_back)  # indexes changed for raw_back as is brought to L2
        nmes = len(raw_data)
        if nband != len(raw_data[0]):
            print("ERROR: different number of pixels between dat and back")
            return None

        # Data conversion
        mesure = raw_data/65535.0
        calibrated_mesure = np.zeros((nmes, nband))
        calibrated_light_measure = np.zeros((nmes, nband))
        back_mesure = np.zeros((nmes, nband))

        for n in range(nmes):
            # Background correction : B0 and B1 read from "back data"
            back_mesure[n, :] = raw_back[:, 0] + raw_back[:, 1]*(int_time[n]/int_time_t0)
            back_corrected_mesure = mesure[n] - back_mesure[n, :]

            # Offset substraction : dark index read from attribute
            offset = np.mean(back_corrected_mesure[DarkPixelStart:DarkPixelStop])
            offset_corrected_mesure = back_corrected_mesure - offset

            # Normalization for integration time
            normalized_mesure = offset_corrected_mesure*int_time_t0/int_time[n]
            normalised_light_measure = back_corrected_mesure*int_time_t0/int_time[n]  # do not do the dark substitution as we need light data

            # Sensitivity calibration
            calibrated_mesure[n, :] = normalized_mesure  # /raw_cal
            calibrated_light_measure[n, :] = normalised_light_measure  # /raw_cal

        # get light and dark data before correction
        light_avg = np.mean(calibrated_light_measure, axis=0)  # [ind_nocal == False]
        light_std = np.std(calibrated_light_measure, axis=0) / pow(nmes, 0.5)  # [ind_nocal == False]

        # ensure all TriOS outputs are length 255 to match SeaBird HyperOCR stats output
        ones = np.ones(nband)  # to provide array of 1s with the correct shape
        dark_avg = ones * offset
        dark_std = ones * np.std(back_corrected_mesure[DarkPixelStart:DarkPixelStop], axis=0) / pow(nmes, 0.5)
        # adjusting the dark_ave and dark_std shapes will remove sensor specific behaviour in Default and Factory

        stdevSignal = {}
        for i, wvl in enumerate(raw_wvl):
            stdevSignal[wvl] = pow(
                (pow(light_std[i], 2) + pow(dark_std[i], 2)), 0.5) / np.average(calibrated_mesure, axis=0)[i]

        return dict(
            ave_Light=np.array(light_avg),
            ave_Dark=np.array(dark_avg),
            std_Light=np.array(light_std),
            std_Dark=np.array(dark_std),
            std_Signal=stdevSignal,
        )

    def FRM(self, node, uncGrp, raw_grps, raw_slices, stats, newWaveBands, refData: dict):
        # TriOS specific
        output = {}
        raw_name = node.attributes['RAW_FILE_NAME']
        cast = raw_name[raw_name.find('FICE22'):raw_name.find('.mlb')]
        stats = None  # stats is unused in this method, but required as an input because of Seabird
        for sensortype in ['ES', 'LI', 'LT']:

            refGrp = np.asarray(list(refData[sensortype].values()), dtype=float).flatten()
            fig = plt.figure(f"{sensortype}_{os.path.basename(cast)}")

            ### Read HDF file inputs
            grp = raw_grps[sensortype]
            # slice = rawSlices[sensortype]
            slice = raw_slices[sensortype]

            # read data for L1B FRM processing
            raw_data = np.asarray(list(slice['data'].values())).transpose()  # raw_data = np.asarray(grp.getDataset(sensortype).data.tolist())
            DarkPixelStart = int(grp.attributes["DarkPixelStart"])
            DarkPixelStop = int(grp.attributes["DarkPixelStop"])
            int_time = np.asarray(grp.getDataset("INTTIME").data.tolist())
            int_time_t0 = int(grp.getDataset("BACK_" + sensortype).attributes["IntegrationTime"])

            ### Read full characterisation files
            radcal_wvl = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['1'][1:].tolist())

            refGrp, _ = self.interp_common_wvls(refGrp,
                                                np.asarray(list(refData[sensortype].keys()), dtype=float),
                                                radcal_wvl)
            ### for masking arrays only
            raw_cal = grp.getDataset(f"CAL_{sensortype}").data

            B0 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['4'][1:].tolist())
            B1 = np.asarray(
                pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['5'][1:].tolist())
            S1 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['6']
            S2 = pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['8']
            mZ = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_LSF").data))
            mZ_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_STRAYDATA_UNCERTAINTY").data))
            mZ = mZ[1:, 1:]  # remove 1st line and column, we work on 255 pixel not 256.
            mZ_unc = mZ_unc[1:, 1:]  # remove 1st line and column, we work on 255 pixel not 256.
            Ct = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data[1:].transpose().tolist())[4])
            Ct_unc = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_TEMPDATA_CAL").data[1:].transpose().tolist())[5])
            # divide LAMP by 10 to account for TriOS unit conversion
            LAMP = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['2'])/10
            LAMP_unc = ((np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_LAMP").data)['3'])/100)*LAMP)

            # Defined constants
            nband = len(B0)
            nmes = len(raw_data)
            grp.attributes["nmes"] = nmes
            n_iter = 5

            # set up uncertainty propagation
            mDraws = 100  # number of monte carlo draws
            prop = punpy.MCPropagation(mDraws, parallel_cores=1)

            # uncertainties from data:
            sample_mZ = cm.generate_sample(mDraws, mZ, mZ_unc, "rand")
            sample_n_iter = cm.generate_sample(mDraws, n_iter, None, None, dtype=int)
            sample_int_time_t0 = cm.generate_sample(mDraws, int_time_t0, None, None)
            sample_LAMP = cm.generate_sample(mDraws, LAMP, LAMP_unc, "syst")
            sample_Ct = cm.generate_sample(mDraws, Ct, Ct_unc, "syst")

            # Non-linearity alpha computation

            t1 = S1.iloc[0]
            S1 = S1.drop(S1.index[0])
            t2 = S2.iloc[0]
            S2 = S2.drop(S2.index[0])
            sample_t1 = cm.generate_sample(mDraws, t1, None, None)

            S1 = np.asarray(S1/65535.0, dtype=float)
            S2 = np.asarray(S2/65535.0, dtype=float)
            k = t1/(t2 - t1)
            sample_k = cm.generate_sample(mDraws, k, None, None)

            S1_unc = (pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['7']/100)[1:]*np.abs(S1)
            S2_unc = (pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_CAL").data)['9']/100)[1:]*np.abs(S2)

            sample_S1 = cm.generate_sample(mDraws, np.asarray(S1), S1_unc, "rand")
            sample_S2 = cm.generate_sample(mDraws, np.asarray(S2), S2_unc, "rand")

            S12 = self.S12func(k, S1, S2)
            sample_S12 = prop.run_samples(self.S12func, [sample_k, sample_S1, sample_S2])

            S12_sl_corr = self.Slaper_SL_correction(S12, mZ, n_iter=5)
            S12_sl_corr_unc = []
            sl4 = self.Slaper_SL_correction(S12, mZ, n_iter=4)
            for i in range(len(S12_sl_corr)):  # get the difference between n=4 and n=5
                if S12_sl_corr[i] > sl4[i]:
                    S12_sl_corr_unc.append(S12_sl_corr[i] - sl4[i])
                else:
                    S12_sl_corr_unc.append(sl4[i] - S12_sl_corr[i])

            sample_S12_sl_syst = cm.generate_sample(mDraws, S12_sl_corr, np.array(S12_sl_corr_unc), "syst")
            sample_S12_sl_rand = prop.run_samples(self.Slaper_SL_correction, [sample_S12, sample_mZ, sample_n_iter])
            sample_S12_sl_corr = prop.combine_samples([sample_S12_sl_syst, sample_S12_sl_rand])

            alpha = self.alphafunc(S1, S12)
            sample_alpha = prop.run_samples(self.alphafunc, [sample_S1, sample_S12])
            # alpha_unc = np.power(np.power(S1_unc, 2) + np.power(S2_unc, 2) + np.power(S2_unc, 2), 0.5)
            # sample_alpha = cm.generate_sample(mDraws, alpha, alpha_unc, "syst")

            # Updated calibration gain
            if sensortype == "ES":
                # for plotting and testing
                updated_radcal_gain = (S12_sl_corr / LAMP) * (int_time_t0 / t1)
                updated_radcal_gain_mf = self.update_cal_ES(S12_sl_corr, LAMP, int_time_t0, t1)

                # avg_coserror, full_hemi_coserror, zenith_ang = ProcessL1b_FRMCal.cosine_error_correction(node,
                #                                                                                          sensortype)

                # Compute avg cosine error (not done for the moment)
                cos_mean_vals, cos_uncertainties = self.prepare_cos(uncGrp, sensortype, 'L2')
                corr = [None, "syst", "syst", "rand"]
                sample_radcal_wvl, sample_coserr, sample_coserr90, sample_zen_ang = [
                    cm.generate_sample(mDraws, samp, cos_uncertainties[i], corr[i]) for i, samp in
                    enumerate(cos_mean_vals)]
                # ZEN_avg_coserror, AZI_avg_coserror, zenith_ang, ZEN_delta_err, ZEN_delta, AZI_delta_err, AZI_delta
                avg_coserror, avg_azi_coserror, zenith_ang, zen_delta, azi_delta, zen_unc, azi_unc = \
                    self.cosine_error_correction(uncGrp, sensortype)
                # two components for cos unc, one from the file (rand), one is the difference between two symmetries

                # error due to lack of symmetry in cosine response
                sample_azi_delta_err1 = cm.generate_sample(mDraws, avg_azi_coserror, azi_unc, "syst")
                sample_azi_delta_err2 = cm.generate_sample(mDraws, avg_azi_coserror, azi_delta, "syst")
                sample_azi_delta_err = prop.combine_samples([sample_azi_delta_err1, sample_azi_delta_err2])
                sample_azi_err = prop.run_samples(self.AZAvg_Coserr, [sample_coserr, sample_coserr90])
                sample_azi_avg_coserror = prop.combine_samples([sample_azi_err, sample_azi_delta_err])

                sample_zen_delta_err1 = cm.generate_sample(mDraws, avg_coserror, zen_unc, "syst")
                sample_zen_delta_err2 = cm.generate_sample(mDraws, avg_coserror, zen_delta, "syst")
                sample_zen_delta_err = prop.combine_samples([sample_zen_delta_err1, sample_zen_delta_err2])
                sample_zen_err = prop.run_samples(self.ZENAvg_Coserr, [sample_radcal_wvl, sample_azi_avg_coserror])
                sample_zen_avg_coserror = prop.combine_samples([sample_zen_err, sample_zen_delta_err])

                full_hemi_coserr = self.FHemi_Coserr(avg_coserror, zenith_ang)
                sample_fhemi_coserr = prop.run_samples(self.FHemi_Coserr, [sample_zen_avg_coserror, sample_zen_ang])

                # Irradiance direct and diffuse ratio
                res_py6s = ProcessL1b_FRMCal.get_direct_irradiance_ratio(node, sensortype, called_L2=True)
                sample_updated_radcal_gain = prop.run_samples(self.update_cal_ES,
                                                              [sample_S12_sl_corr, sample_LAMP, sample_int_time_t0,
                                                               sample_t1])
            else:
                PANEL = np.asarray(pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['2'])
                unc_PANEL = (np.asarray(
                    pd.DataFrame(uncGrp.getDataset(sensortype + "_RADCAL_PANEL").data)['3'])/100)*PANEL
                sample_PANEL = cm.generate_sample(mDraws, PANEL, unc_PANEL, "syst")

                # plotting and testing
                updated_radcal_gain = (np.pi * S12_sl_corr) / (LAMP * PANEL) * (int_time_t0 / t1)

                sample_updated_radcal_gain = prop.run_samples(self.update_cal_rad,
                                                              [sample_PANEL, sample_S12_sl_corr, sample_LAMP,
                                                               sample_int_time_t0, sample_t1])

            # Data conversion
            mesure = raw_data/65535.0

            back_mesure = np.array([B0 + B1*(int_time[n]/int_time_t0) for n in range(nmes)])
            back_corrected_mesure = mesure - back_mesure
            std_light = np.std(back_corrected_mesure, axis=0)/np.power(nmes, 0.5)
            sample_back_corrected_mesure = cm.generate_sample(mDraws, np.mean(back_corrected_mesure, axis=0), std_light,
                                                              "rand")
            back_unc = prop.process_samples(None, sample_back_corrected_mesure)
            # Offset substraction : dark index read from attribute
            offset = np.mean(back_corrected_mesure[:, DarkPixelStart:DarkPixelStop], axis=1)
            offset_corrected_mesure = np.asarray(
                [back_corrected_mesure[:, i] - offset for i in range(nband)]).transpose()
            # get standard deviation across scans
            offset_std = np.mean(np.std(back_corrected_mesure[:, DarkPixelStart:DarkPixelStop], axis=0))
            # get standard deviation across dark pixels
            offset_std_scans = np.mean(np.std(back_corrected_mesure[:, DarkPixelStart:DarkPixelStop], axis=1))
            std_dark = np.power((np.power(offset_std_scans, 2) + np.power(offset_std, 2)), 0.5)/np.power(nmes, 0.5)

            # add in quadrature with std in offset across scans
            sample_offset = cm.generate_sample(mDraws, np.mean(offset), np.mean(std_dark), "rand")
            sample_offset_corrected_mesure = prop.run_samples(self.dark_Substitution,
                                                              [sample_back_corrected_mesure, sample_offset])

            # average the signal and int_time for the station
            offset_corr_mesure = np.mean(offset_corrected_mesure, axis=0)
            int_time = np.average(int_time)

            prop = punpy.MCPropagation(mDraws, parallel_cores=1)

            back_unc_rel = np.abs((back_unc * 1e10)/(refGrp * 1e10))*100
            plt.plot(radcal_wvl, back_unc_rel, label="Environmental Instability")
            offset_unc = prop.process_samples(None, sample_offset_corrected_mesure)
            std_rel = np.abs((offset_unc * 1e10)/(refGrp * 1e10))*100
            plt.plot(radcal_wvl, std_rel, label="+Noise")
            # set standard variables
            n_iter = 5
            sample_n_iter = cm.generate_sample(mDraws, n_iter, None, None, dtype=int)

            # Non-Linearity Correction
            linear_corr_mesure = self.non_linearity_corr(offset_corr_mesure, alpha)
            sample_linear_corr_mesure = prop.run_samples(self.non_linearity_corr,
                                                         [sample_offset_corrected_mesure, sample_alpha])

            nlin_unc = prop.process_samples(None, sample_linear_corr_mesure)
            nlin_unc_rel = np.abs((nlin_unc * 1e10) / (refGrp * 1e10))*100  # linear_corr_mesure
            plt.plot(radcal_wvl, nlin_unc_rel, label="+nLin")

            # Straylight Correction
            straylight_corr_mesure = self.Slaper_SL_correction(linear_corr_mesure, mZ, n_iter)

            S12_sl_corr_unc = []
            sl4 = self.Slaper_SL_correction(linear_corr_mesure, mZ, n_iter=4)
            for i in range(len(straylight_corr_mesure)):  # get the difference between n=4 and n=5
                if linear_corr_mesure[i] > sl4[i]:
                    S12_sl_corr_unc.append(straylight_corr_mesure[i] - sl4[i])
                else:
                    S12_sl_corr_unc.append(sl4[i] - straylight_corr_mesure[i])

            sample_straylight_1 = cm.generate_sample(mDraws, straylight_corr_mesure, np.array(S12_sl_corr_unc), "syst")
            sample_straylight_2 = prop.run_samples(self.Slaper_SL_correction,
                                                   [sample_linear_corr_mesure, sample_mZ, sample_n_iter])
            sample_straylight_corr_mesure = prop.combine_samples([sample_straylight_1, sample_straylight_2])

            # Normalization Correction, based on integration time
            normalized_mesure = straylight_corr_mesure * int_time_t0 / int_time
            sample_normalized_mesure = sample_straylight_corr_mesure*int_time_t0/int_time
            norm_unc = prop.process_samples(None, sample_normalized_mesure)

            ## PLOT NORMALISATION UNCERTAINTY - COMBINES STRAYLIGHT
            rel_norm_unc = (norm_unc * 1e10 / refGrp * 1e10) * 100  # normalized_mesure
            plt.plot(radcal_wvl, rel_norm_unc, label="+Straylight")

            # Calculate New Calibration Coeffs
            calibrated_mesure = normalized_mesure / updated_radcal_gain
            sample_calibrated_mesure = prop.run_samples(self.absolute_calibration,
                                                        [sample_normalized_mesure, sample_updated_radcal_gain])

            # Thermal correction
            thermal_corr_mesure = Ct * calibrated_mesure
            sample_thermal_corr_mesure = prop.run_samples(self.thermal_corr, [sample_Ct, sample_calibrated_mesure])
            therm_unc = prop.process_samples(None, sample_thermal_corr_mesure)
            therm_unc_rel = np.abs((therm_unc * 1e10) / (refGrp * 1e10)) * 100
            plt.plot(radcal_wvl, therm_unc_rel, label="+Thermal")

            if sensortype.lower() == "es":
                # get cosine correction attributes and samples from dictionary
                solar_zenith = res_py6s['solar_zenith']
                direct_ratio = res_py6s['direct_ratio']

                sample_sol_zen = cm.generate_sample(mDraws, solar_zenith, 0.05, "rand")
                sample_dir_rat = cm.generate_sample(mDraws, direct_ratio, 0.08*direct_ratio, "syst")

                sample_cos_corr_mesure = prop.run_samples(self.cosine_corr,
                                                          [sample_zen_avg_coserror, sample_fhemi_coserr, sample_zen_ang,
                                                           sample_thermal_corr_mesure, sample_sol_zen, sample_dir_rat])

                unc = prop.process_samples(None, sample_cos_corr_mesure)
                sample = sample_cos_corr_mesure

                ind_closest_zen = np.argmin(np.abs(zenith_ang-solar_zenith))
                cos_corr = 1-avg_coserror[:,ind_closest_zen]/100
                Fhcorr = 1-full_hemi_coserr/100
                cos_corr_mesure = (direct_ratio * thermal_corr_mesure * cos_corr) + (
                            (1 - direct_ratio) * thermal_corr_mesure * Fhcorr)

                cos_unc_rel = np.abs((unc * 1e10)/(refGrp * 1e10))*100  # unc in %
                plt.plot(radcal_wvl, cos_unc_rel, label="+Cosine")
                # unc = cos_unc
            else:
                sample = sample_thermal_corr_mesure
                unc = therm_unc

            # mask for arrays
            ind_zero = np.array([rc[0] == 0 for rc in raw_cal])  # changed due to raw_cal now being a np array
            ind_nan = np.array([np.isnan(rc[0]) for rc in raw_cal])
            ind_nocal = ind_nan | ind_zero

            # Remove wvl without calibration from the dataset and make uncertainties relative
            output[f"{sensortype.lower()}Wvls"] = radcal_wvl[ind_nocal == False]
            output[
                f"{sensortype.lower()}Unc"] = unc[ind_nocal == False]  # dict(zip(str_wvl[self.ind_nocal==False], filtered_unc))  # unc in dict with wavelengths
            output[f"{sensortype.lower()}Sample"] = sample[:, ind_nocal == False]  # samples keep raw

            # setup plot settings and save
            plt.xlim((350, 900))
            plt.ylim((0, 5))
            plt.title(f"{os.path.basename(cast)}: {sensortype} - Breakdown FRM")
            plt.legend()
            plt.savefig(f"{sensortype}_{os.path.basename(cast)}_breakdown.jpg")

        for sensortype in ['ES', 'LI', 'LT']:
            # get sensor specific wavebands - output[f"{sensortype.lower()}Wvls"].pop
            wvls = np.asarray(output.pop(f"{sensortype.lower()}Wvls"), dtype=float)
            _, output[f"{sensortype.lower()}Unc"] = self.interp_common_wvls(
                output[f"{sensortype.lower()}Unc"], wvls, newWaveBands)
            output[f"{sensortype.lower()}Sample"] = self.interpolateSamples(
                output[f"{sensortype.lower()}Sample"], wvls, newWaveBands)

        return output  # return products as dictionary to be appended to xSlice

    # Measurement functions
    @staticmethod
    def back_Mesure(B0, B1, int_time, t0):
        return B0 + B1*(int_time/t0)

    @staticmethod
    def update_cal_ES(S12_sl_corr, LAMP, int_time_t0, t1):
        updated_radcal_gain = (S12_sl_corr/LAMP)*(int_time_t0/t1)
        # sensitivity factor : if gain==0 (or NaN), no calibration is performed and data is affected to 0
        ind_zero = (updated_radcal_gain <= 1e-2)
        ind_nan = np.isnan(updated_radcal_gain)
        ind_nocal = ind_nan | ind_zero
        updated_radcal_gain[ind_nocal == True] = 1  # set 1 instead of 0 to perform calibration (otherwise division per 0)
        return updated_radcal_gain

    @staticmethod
    def update_cal_rad(PANEL, S12_sl_corr, LAMP, int_time_t0, t1):
        updated_radcal_gain = (np.pi*S12_sl_corr)/(LAMP*PANEL)*(int_time_t0/t1)

        # sensitivity factor : if gain==0 (or NaN), no calibration is performed and data is affected to 0
        ind_zero = (updated_radcal_gain <= 1e-2)
        ind_nan = np.isnan(updated_radcal_gain)
        ind_nocal = ind_nan | ind_zero
        updated_radcal_gain[
            ind_nocal == True] = 1  # set 1 instead of 0 to perform calibration (otherwise division per 0)
        return updated_radcal_gain
